## **TIL**
### **RNN(Recurrent Neural Network)이란?**
> RNN은 시계열 또는 순차 데이터를 예측하는 딥러닝을 위한 신경망 아키텍처입니다.<br> RNN은 다양한 길이의 순차 데이터로 작업하고 자연 신호 분류, 언어 처리, 비디오 분석 등의 문제를 해결하는 데 특히 효과적입니다.

**RNN의 작동 방식**
> RNN은 과거의 정보를 사용하여 현재 및 미래의 입력에 대한 신경망의 성능을 개선하는 딥러닝 구조입니다. RNN의 독특한 점은 신경망에 은닉 상태 및 루프가 있다는 것입니다. 루프 구조를 통해 신경망은 은닉 상태에 과거의 정보를 저장하고 시퀀스에 대해 연산할 수 있습니다.

> RNN은 과거의 정보를 현재 입력에 어떻게 적용할까요? 이 신경망에는 두 개의 가중치 세트가 있습니다. 하나는 은닉 상태 벡터에 대한 가중치 세트이며, 다른 하나는 입력에 대한 가중치 세트입니다. 신경망은 훈련 중에 입력과 은닉 상태, 두 가지 모두에 대한 가중치를 학습합니다. 구현이 되면 출력은 현재 입력, 그리고 이전 입력을 기반으로 하는 은닉 상태를 기반으로 합니다.


**RNN의 기본 구조**
<img src = "http://i.imgur.com/Q8zv6TQ.png">

>RNN은 히든 노드가 방향을 가진 엣지로 연결돼 순환구조를 이루는(directed cycle) 인공신경망의 한 종류입니다. 음성, 문자 등 순차적으로 등장하는 데이터 처리에 적합한 모델로 알려져 있는데요. Convolutional Neural Networks(CNN)과 더불어 최근 들어 각광 받고 있는 알고리즘입니다.
  * 위의 그림에서도 알 수 있듯 시퀀스 길이에 관계없이 인풋과 아웃풋을 받아들일 수 있는 네트워크 구조이기 때문에 필요에 따라 다양하고 유연하게 구조를 만들 수 있다는 점이 RNN의 가장 큰 장점입니다.

<img src = "https://kr.mathworks.com/discovery/rnn/_jcr_content/mainParsys3/discoverysubsection/mainParsys3/image.adapt.full.medium.jpg/1701674213137.jpg" style = "width: 70%;">

> RNN의 셀 하나를 펼친 모습. 데이터 시퀀스에 대해 정보가 신경망 안에서 어떻게 움직이는지 볼 수 있습니다. 입력은 셀의 은닉 상태에 의해 계산되어 출력을 생성하고, 은닉 상태는 다음 시간 스텝으로 전달됩니다.

---

### **LSTM**

<img src = "http://i.imgur.com/H9UoXdC.png">

> RNN의 기울기 소실 문제점을 표현한 그림

* RNN은 관련 정보와 그 정보를 사용하는 지점 사이 거리가 멀 경우 역전파시 그래디언트가 점차 줄어 학습능력이 크게 저하되는 것으로 알려져 있습니다. 이를 vanishing gradient problem이라고 합니다.

* RNN은 일반적으로 역전파를 통해 훈련되며, 여기서 “소실” 또는 “폭주”하는 기울기 문제에 직면할 수 있습니다. 이러한 문제로 인해 신경망의 가중치가 아주 작아지거나 아주 커지므로 장기적인 관계를 학습하기에는 효율성이 떨어지게 됩니다.

* 이 문제를 극복하기 위해서 고안된 특수한 RNN 유형이 LSTM(장단기 기억) 신경망입니다. LSTM은 RNN의 히든 state에 cell-state를 추가한 구조입니다.

*  LSTM 신경망은 부가적인 게이트를 사용하여 은닉 셀의 어느 정보가 출력과 다음 은닉 상태까지 보내지는지를 제어합니다. 이를 통해 신경망은 데이터의 장기적인 관계를 더 효과적으로 학습할 수 있습니다. LSTM은 일반적으로 구현되는 RNN 유형입니다.

**LSTM의 기본 구조**

<img src = "http://i.imgur.com/jKodJ1u.png">

* cell state는 일종의 컨베이어 벨트 역할을 합니다. 덕분에 state가 꽤 오래 경과하더라도 그래디언트가 비교적 전파가 잘 되게 됩니다. LSTM 셀의 수식은 아래와 같습니다. ⊙는 요소별 곱셈을 뜻하는 Hadamard product 연산자입니다.

$ftitotgtctht=σ(Wxh_fxt+Whh_fht−1+bh_f)=σ(Wxh_ixt+Whh_iht−1+bh_i)=σ(Wxh_oxt+Whh_oht−1+bh_o)=tanh(Wxh_gxt+Whh_ght−1+bh_g)=ft⊙ct−1+it⊙gt=ot⊙tanh(ct)$

> RNN, LSTM은 시계열 데이터를 다루는 데 유용한 신경망 구조입니다. 이들은 이전 시간 단계의 정보를 현재 시간 단계에 전달하여 순서가 있는 데이터를 처리할 수 있습니다. 하지만 현업에서 사용하기에는 한계가 있습니다.
