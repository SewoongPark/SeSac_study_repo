{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcb92VeqS3AzMu48ai4hPZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SewoongPark/SeSac_study_repo/blob/main/study_00.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TIL**\n",
        "* Pytorch로 딥러닝 기본 복습하기\n",
        "* 딥러닝에 사용되는 Dense 구성해보기,\n",
        "* epoch, optimizer, gradient desecent 만들어보기\n",
        "\n",
        "### **Optimizer**\n",
        "> 딥러닝(Deep Learning)은 입력층(Input Layer)과 출력층(Output Layer) 사이에 여러 은닉층(Hidden Layer)으로 이루어진 인공 신경망입니다.<br>\n",
        "층에 깊이가 깊고 복잡해질수록 Hyperparameter 또한 많아지게 됩니다. 이는 모델의 학습 속도나 성능에 직접적인 영향을 주게 되므로, 이를 잘 결정하여 모델이 원하는 결과를 낼 수 있도록 하는 것이 딥러닝 학습의 핵심입니다.\n",
        "\n",
        " > <br> 학습하며 얼마나 틀렸지를 Loss 라고 하며, 이에 대한 스코어를 반환하는 함수를 Loss Function이라고 합니다.<br>\n",
        "딥러닝에서 학습의 목표는 이 Loss Function의 최솟값을 찾는 것입니다. 이 과정을 Optimization이라고 하며, 최적화라고 부르기도 합니다. 이는 학습을 빠르고 안정적으로 하는 것이 목표입니다.\n",
        "그리고 이를 수행하는 알고리즘이 Optimizer입니다. Optimizer는 여러 종류가 있으며, 그 종류에 따라 Loss의 최저점을 찾아가는 방식이 다릅니다.\n",
        "\n",
        "<img src = \"https://miro.medium.com/v2/resize:fit:640/format:webp/1*Y2KPVGrVX9MQkeI8Yjy59Q.gif\">\n",
        "\n",
        "* 출처: https://medium.com/cdri/optimizer%EC%97%90-%EB%8C%80%ED%95%9C-%EC%A0%84%EB%B0%98%EC%A0%81%EC%9D%B8-%EC%9D%B4%ED%95%B4-633d8ec9ac1b\n",
        "\n",
        "---\n",
        "\n",
        "> * Optimizer는 Learning rate나 Gradient를 어떻게 할 지에 따라 종류가 다양합니다.\n",
        "GD는 Gradient Descent이고, SGD는 Stochastic Gradient Descent입니다.\n",
        "\n",
        "<img src = \"https://miro.medium.com/v2/resize:fit:720/format:webp/1*YcOyk3IkjRW1LZLxiVWUnQ.png\">\n",
        "\n",
        "> ## $$w:=w−α∇J(w)$$\n",
        "\n",
        "\n",
        "*   $w$는 업데이트할 매개 변수를 나타냅니다.\n",
        "*   $α$는 학습률(learning rate)로, 각 단계에서 얼마나 매개 변수를 조정할지 결정합니다.\n",
        "*   $α∇J(w)$는 손실 함수 $J(w)$의 gradient입니다.\n",
        "\n",
        "> 이 갱신 규칙은 현재 위치에서의 gradient 방향으로 학습률에 따라 적절한 거리만큼 이동하여 매개 변수를 갱신합니다. 이 과정을 여러 번 반복하여 손실 함수를 최소화하는 최적의 매개 변수를 찾습니다.\n",
        "\n",
        "> **Learning rate**는 한 번에 얼마나 학습할지, Gradient는 어떤 방향으로 학습할지를 나타냅니다. Optimizer의 차이점은 이에서 비롯되며, 이를 수정하며 발전합니다.\n",
        "RMSProp, Adagrad, AdaDelta는 Learning rate를 수정한 Optimizer이고, Nag, Momentum은 Gradient를 수정하였습니다.\n",
        "두 분류의 장점을 모두 가진 Optimizer는 **Adam, Nadam** 입니다.\n",
        "\n",
        "\n",
        "### **training data의 back propagation**\n",
        "  * data size / batch size만큼의 epoch를 도는 동안, 한 epoch가 끝나면 오차를 계산하고 역전파 작업을 수행합니다.\n",
        "\n",
        "### **ANN 및 퍼셉트론의 기본 이론**\n",
        ">  * $AND, OR, NOT, XOR$등의 논리 게이트의 y 출력 값을 계산하기 위해\n",
        "$X_n$의 값을 조절한다.\n",
        "  <img src = \"https://blog.kakaocdn.net/dn/c5gUSA/btqVddxpJTc/ENmE5C7wMicrOuB6BqMVlK/img.png\">\n",
        "  \n",
        "  > * $y$값이 맞지 않는 오차가 생김 -> $w$값 재계산\n",
        "  > * learning rate 값이 클 수록 이전 값과의 갱신폭 차이가 크다\n",
        "  \n",
        "  * 오차 계산과 가중치 업데이트: 출력 값이 실제 값과 일치하지 않을 때 오차가 발생하고, 이 오차를 최소화하기 위해 가중치를 조정합니다. 이 과정은 보통 경사 하강법을 사용하여 수행되며, 학습률(learning rate)은 이전 값과의 갱신 폭의 차이를 결정합니다.\n",
        "  \n",
        "  * 학습률(learning rate): 학습률은 매개 변수를 업데이트할 때의 보폭을 결정하는 하이퍼파라미터입니다. 학습률이 클수록 각 업데이트 단계에서 가중치가 크게 변화하며, 작을수록 변화가 더 부드럽게 이루어집니다. 너무 작은 학습률은 학습 속도를 늦출 수 있고, 너무 큰 학습률은 수렴을 방해할 수 있으므로 적절한 값을 선택해야 합니다.\n",
        "  \n",
        "  * 퍼셉트론은 하나의 선형 결정 경계를 학습하고 XOR과 같은 비선형 문제를 해결할 수 없는 반면, 다층 퍼셉트론(MLP)은 여러 개의 은닉층을 사용하여 복잡한 패턴을 학습할 수 있습니다. XOR과 같은 비선형 문제를 해결하기 위해 MLP가 사용됩니다."
      ],
      "metadata": {
        "id": "vUEh4Dg-BfGG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEzWHpHLBYTt"
      },
      "outputs": [],
      "source": []
    }
  ]
}