## **TIL**
* **빈도수 분석에서의 데이터 구조**
    * 리스트와 array의 차이: 리스트 - 데이터의 삽입/삭제가 비교적 쉽다.
    * 빈도수 분석에서 `null`, `na` 데이터의 비율이 가장 많은 경우가 있음
    * 데이터를 제거한다: 그 자리를 `null`값으로 대체한다.

* 데이터 구조화 예시 
    * `(자료1.split("\n"))`<br> 
        * ["환경을 위해서 오전  출근 시간에 탄소배출권을 실시하겠습니다."] <br>
        * ["육아 휴직을 장려하겠습니다."] \n

    * `자료2 = noun_extractor.extract(자료1)`
        * [[환경, 오전, 출근, 시간, 탄소, 배출],[육아, 휴직, 장려]]

    * **연관성 분석을 위해서는 자료1을 거쳐 자료2의 형태로 변환해주어야 함!** 

    * 자료2에서 전처리(불용어 치환 등)
        * 워드 클라우드로 만들기 위해서는 `1행 n열의 리스트`로 작업
        * `[환경, 오전, 출근, co2, 배출, 육아, 장려]` --> 단어, 빈도 수
            * `환경: 1, 오전: 1 ...`
### **Okt(): 한국어 형태소 분석기**
* OKT는 한국어 자연어 처리를 위한 Python 라이브러리로, 형태소 분석, 품사 태깅, 의존 구문 분석 등 다양한 기능을 제공

* **OKT에서 제공하는 품사 태그**

명사

    * NN: 일반 명사
    * NP: 고유 명사
    * NR: 수사 명사
    * NX: 조사가 붙은 명사
형용사

    * VA: 형용사
    * VN: 명사형 형용사
동사

    * VV: 동사
    * VX: 조사가 붙은 동사
부사

    * MAG: 일반 부사
    * MAJ: 조사가 붙은 부사
조사

    * JC: 조사
관형사

    * JK: 관형사
접속사

    * JC: 접속사
기호

    * SF: 문장 부호
    * SS: 특수 기호
    * SP: 띄어쓰기    
  * document 열에 중복된 id값이 있음을 확인
* 하나의 아이디로 여러개의 값을 작성하는 것은 어쩔 수 없는 부분이기 때문에 중복 제거 x

* 예를 들어 document가 반드시 필요하면 id 값을 찾아서 연락하면 됨

* 기술적인 부분: document 열에 null 값이 있으면 안돼서 pandas 내의 함수를 사용해서 null 데이터와 중복 데이터를 조회했다. 

### **SOYNLP의 응집 확률(cohesion probability)**
* **Chain rule**
* 예) "반포한강공원"에 "반포한강공원"의 출현 빈도는 얼마나 되는가


<img src = "https://wikidocs.net/images/page/92961/%EC%88%98%EC%8B%9D.png"></img>

<img src = "https://wikidocs.net/images/page/92961/%EC%88%98%EC%8B%9D2.png">

* **Bag of Words** : 단어의 출현 빈도로만 텍스트 데이터를 수치화 하는 방법(문맥 고려x)
* **언어 모델(Language Model)**
* **원-핫 인코딩(One-Hot Encoding)**

* **케라스 전처리 도구로 패딩하기**

* **문서 단어 행렬(Document-Term Matrix, DTM)**
    > 문서 단어 행렬(Document-Term Matrix, DTM)이란 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것을 말합니다.
    <br>**문서에서 단어 등장한 횟수가 TF(Term Frequency)**

    ![image.png](attachment:image.png)

* **TF-IDF(단어 빈도-역 문서 빈도, Term Frequency-Inverse Document Frequency)**
    > * TF-IDF(Term Frequency-Inverse Document Frequency)는 단어의 빈도와 역 문서 빈도(문서의 빈도에 특정 식을 취함)를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법입니다. 우선 DTM을 만든 후, TF-IDF 가중치를 부여합니다.

    > * TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단합니다. TF-IDF 값이 낮으면 중요도가 낮은 것이며, TF-IDF 값이 크면 중요도가 큰 것입니다. 즉, the나 a와 같이 불용어의 경우에는 모든 문서에 자주 등장하기 마련이기 때문에 자연스럽게 불용어의 TF-IDF의 값은 다른 단어의 TF-IDF에 비해서 낮아지게 됩니다.

    
* TF-IDF에서 IDF는 **역 문서 빈도(Inverse Document Frequency)** 의 약자로, 특정 단어가 전체 문서 집합에서 얼마나 흔한지를 나타내는 지표입니다.

* IDF 값 의미:
    * 높은 IDF 값: 특정 단어가 드물게 등장하여 중요도가 높음을 의미합니다.
    * 낮은 IDF 값: 특정 단어가 자주 등장하여 중요도가 낮음을 의미합니다.

* 예시:
    * "인공지능"이라는 단어가 100개 문서 중 10개 문서에만 등장하면 IDF 값이 높아집니다.
    * "the"라는 단어가 100개 문서 중 90개 문서에 등장하면 IDF 값이 낮아집니다.
